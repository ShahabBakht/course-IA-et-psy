{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Classification Lin√©aire en Pratique\n",
    "\n",
    "## PSY3913/6913 - IA, Psychologie et Neuroscience Cognitive\n",
    "\n",
    "Dans ce notebook, nous allons explorer la classification lin√©aire (r√©gression logistique) :\n",
    "1. Comprendre la fonction sigmo√Øde\n",
    "2. Impl√©menter la r√©gression logistique from scratch\n",
    "3. Visualiser les fronti√®res de d√©cision\n",
    "4. Utiliser scikit-learn\n",
    "5. Classification multi-classe\n",
    "6. Application en neurosciences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour des graphiques plus jolis\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Pour la reproductibilit√©\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprendre la fonction sigmo√Øde\n",
    "\n",
    "La fonction sigmo√Øde est au c≈ìur de la r√©gression logistique. Elle transforme n'importe quelle valeur r√©elle en une valeur entre 0 et 1.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Fonction sigmo√Øde.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : float or array-like\n",
    "        Valeur(s) d'entr√©e\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float or array-like\n",
    "        Valeur(s) entre 0 et 1\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Visualisation de la fonction sigmo√Øde\n",
    "z_values = np.linspace(-10, 10, 200)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z_values, sigmoid_values, linewidth=2, label='œÉ(z)')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Seuil de d√©cision (0.5)')\n",
    "plt.axvline(x=0, color='g', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('z = Wx + b', fontsize=12)\n",
    "plt.ylabel('œÉ(z)', fontsize=12)\n",
    "plt.title('Fonction Sigmo√Øde', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim([-0.1, 1.1])\n",
    "\n",
    "# Annoter des points cl√©s\n",
    "plt.annotate('œÉ(0) = 0.5', xy=(0, 0.5), xytext=(2, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            fontsize=11)\n",
    "plt.annotate('œÉ(‚àû) ‚Üí 1', xy=(8, sigmoid(8)), xytext=(5, 0.85),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            fontsize=11)\n",
    "plt.annotate('œÉ(-‚àû) ‚Üí 0', xy=(-8, sigmoid(-8)), xytext=(-5, 0.15),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            fontsize=11)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Propri√©t√©s de la sigmo√Øde:\")\n",
    "print(\"   ‚Ä¢ Sortie toujours entre 0 et 1 ‚Üí peut √™tre interpr√©t√©e comme probabilit√©\")\n",
    "print(\"   ‚Ä¢ œÉ(0) = 0.5 ‚Üí point de d√©cision\")\n",
    "print(\"   ‚Ä¢ Forme en 'S' ‚Üí transition douce entre les classes\")\n",
    "print(f\"   ‚Ä¢ œÉ(5) = {sigmoid(5):.4f} ‚Üí haute confiance pour classe 1\")\n",
    "print(f\"   ‚Ä¢ œÉ(-5) = {sigmoid(-5):.4f} ‚Üí haute confiance pour classe 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cr√©er des donn√©es synth√©tiques 2D\n",
    "\n",
    "Cr√©ons un dataset simple avec 2 features et 2 classes lin√©airement s√©parables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es synth√©tiques\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1,\n",
    "                           class_sep=1.5, random_state=42)\n",
    "\n",
    "print(f\"üìä Dataset cr√©√©:\")\n",
    "print(f\"   Forme de X: {X.shape}\")\n",
    "print(f\"   Forme de y: {y.shape}\")\n",
    "print(f\"   Classes: {np.unique(y)}\")\n",
    "print(f\"   Distribution: Classe 0: {np.sum(y==0)}, Classe 1: {np.sum(y==1)}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Classe 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Classe 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Dataset pour classification binaire', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: Les deux classes sont relativement bien s√©parables.\")\n",
    "print(\"   Une ligne devrait pouvoir les s√©parer efficacement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Impl√©mentation from scratch : R√©gression logistique\n",
    "\n",
    "Impl√©mentons la r√©gression logistique avec la descente de gradient.\n",
    "\n",
    "**Rappel :**\n",
    "- Mod√®le : $\\hat{y} = \\sigma(Wx + b)$\n",
    "- Loss : $L = -\\frac{1}{n} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n",
    "- Gradients : \n",
    "  - $\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(\\hat{y} - y)$\n",
    "  - $\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum (\\hat{y} - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    \"\"\"\n",
    "    R√©gression logistique avec descente de gradient.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Fonction sigmo√Øde.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def compute_loss(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculer la binary cross-entropy loss.\n",
    "        \n",
    "        Ajoute un petit epsilon pour √©viter log(0).\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Entra√Æner le mod√®le.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialisation\n",
    "        self.w = np.zeros((n_features, 1))\n",
    "        self.b = 0\n",
    "        \n",
    "        # Reshape y\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Descente de gradient\n",
    "        for i in range(self.n_iterations):\n",
    "            # Pr√©diction\n",
    "            z = X.dot(self.w) + self.b\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Calcul de la loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            dw = (1 / n_samples) * X.T.dot(y_pred - y)\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Mise √† jour des param√®tres\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Affichage occasionnel\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"It√©ration {i+1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Pr√©dire les probabilit√©s.\n",
    "        \"\"\"\n",
    "        z = X.dot(self.w) + self.b\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Pr√©dire les classes.\n",
    "        \"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        return (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Retourner les param√®tres.\n",
    "        \"\"\"\n",
    "        return {'w': self.w, 'b': self.b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et entra√Æner le mod√®le\n",
    "model_scratch = LogisticRegressionGD(learning_rate=0.1, n_iterations=1000)\n",
    "print(\"Entra√Ænement du mod√®le...\\n\")\n",
    "model_scratch.fit(X, y)\n",
    "\n",
    "# R√©cup√©rer les param√®tres\n",
    "params = model_scratch.get_params()\n",
    "print(f\"\\nüìä Param√®tres appris:\")\n",
    "print(f\"   w1 = {params['w'][0, 0]:.4f}\")\n",
    "print(f\"   w2 = {params['w'][1, 0]:.4f}\")\n",
    "print(f\"   b = {params['b']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de la courbe d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model_scratch.loss_history, linewidth=2)\n",
    "plt.xlabel('It√©ration', fontsize=12)\n",
    "plt.ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "plt.title('Courbe d\\'apprentissage - Classification', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: La loss diminue et converge.\")\n",
    "print(\"   Le mod√®le apprend √† s√©parer les deux classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de la fronti√®re de d√©cision\n",
    "\n",
    "La fronti√®re de d√©cision est l'ensemble des points o√π $\\hat{y} = 0.5$, c'est-√†-dire o√π $w_1 x_1 + w_2 x_2 + b = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Fronti√®re de d√©cision\"):\n",
    "    \"\"\"\n",
    "    Visualiser la fronti√®re de d√©cision.\n",
    "    \"\"\"\n",
    "    # Cr√©er une grille de points\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                           np.linspace(x2_min, x2_max, 100))\n",
    "    \n",
    "    # Pr√©dire sur tous les points de la grille\n",
    "    X_grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    Z = model.predict(X_grid)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, levels=1, colors=['blue', 'red'])\n",
    "    plt.contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Points de donn√©es\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Classe 0', \n",
    "                alpha=0.7, edgecolors='k', s=50)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Classe 1', \n",
    "                alpha=0.7, edgecolors='k', s=50)\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser\n",
    "plot_decision_boundary(model_scratch, X, y, \n",
    "                      title=\"Fronti√®re de d√©cision (from scratch)\")\n",
    "\n",
    "print(\"\\nüí° Observation: La ligne noire s√©pare les deux r√©gions.\")\n",
    "print(\"   Les r√©gions bleue et rouge correspondent aux pr√©dictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âvaluation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "y_pred = model_scratch.predict(X).flatten()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"üìà Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(f\"\\nüìä Matrice de confusion:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Classe 0', 'Classe 1'],\n",
    "            yticklabels=['Classe 0', 'Classe 1'],\n",
    "            cbar_kws={'label': 'Nombre'})\n",
    "plt.xlabel('Pr√©diction', fontsize=12)\n",
    "plt.ylabel('Vraie classe', fontsize=12)\n",
    "plt.title('Matrice de confusion', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Interpr√©tation:\")\n",
    "print(f\"   ‚Ä¢ Vrais N√©gatifs (TN): {cm[0,0]}\")\n",
    "print(f\"   ‚Ä¢ Faux Positifs (FP): {cm[0,1]}\")\n",
    "print(f\"   ‚Ä¢ Faux N√©gatifs (FN): {cm[1,0]}\")\n",
    "print(f\"   ‚Ä¢ Vrais Positifs (TP): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utilisation de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et entra√Æner le mod√®le sklearn\n",
    "model_sklearn = LogisticRegression(random_state=42)\n",
    "model_sklearn.fit(X, y)\n",
    "\n",
    "# Param√®tres\n",
    "print(f\"üìä Param√®tres scikit-learn:\")\n",
    "print(f\"   w1 = {model_sklearn.coef_[0, 0]:.4f}\")\n",
    "print(f\"   w2 = {model_sklearn.coef_[0, 1]:.4f}\")\n",
    "print(f\"   b = {model_sklearn.intercept_[0]:.4f}\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_sklearn = model_sklearn.predict(X)\n",
    "accuracy_sklearn = accuracy_score(y, y_pred_sklearn)\n",
    "\n",
    "print(f\"\\nüìà Accuracy (scikit-learn): {accuracy_sklearn:.4f} ({accuracy_sklearn*100:.2f}%)\")\n",
    "\n",
    "# Visualiser la fronti√®re de d√©cision\n",
    "plot_decision_boundary(model_sklearn, X, y, \n",
    "                      title=\"Fronti√®re de d√©cision (scikit-learn)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapport de classification d√©taill√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Rapport de classification (scikit-learn):\\n\")\n",
    "print(classification_report(y, y_pred_sklearn, \n",
    "                          target_names=['Classe 0', 'Classe 1']))\n",
    "\n",
    "print(\"\\nüí° Explication des m√©triques:\")\n",
    "print(\"   ‚Ä¢ Precision: Proportion de pr√©dictions positives correctes\")\n",
    "print(\"   ‚Ä¢ Recall: Proportion de vrais positifs correctement identifi√©s\")\n",
    "print(\"   ‚Ä¢ F1-score: Moyenne harmonique de precision et recall\")\n",
    "print(\"   ‚Ä¢ Support: Nombre d'√©chantillons dans chaque classe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification multi-classe\n",
    "\n",
    "### Exemple : Classification de directions de mouvement (neurosciences)\n",
    "\n",
    "Simulons un probl√®me de classification de 4 directions de mouvement √† partir de l'activit√© neuronale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler des donn√©es multi-classes\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trials = 400  # Nombre d'essais\n",
    "n_neurons = 20  # Nombre de neurones\n",
    "n_classes = 4   # 4 directions: droite, gauche, haut, bas\n",
    "\n",
    "# G√©n√©rer les donn√©es\n",
    "X_multi = np.random.randn(n_trials, n_neurons) * 10 + 30  # Firing rates\n",
    "y_multi = np.random.randint(0, n_classes, n_trials)\n",
    "\n",
    "# Ajouter des patterns pour chaque classe\n",
    "for i in range(n_classes):\n",
    "    mask = y_multi == i\n",
    "    # Chaque classe a un pattern distinct\n",
    "    X_multi[mask, i*5:(i+1)*5] += np.random.randn(np.sum(mask), 5) * 5 + 20\n",
    "\n",
    "print(f\"üìä Dataset multi-classe:\")\n",
    "print(f\"   Nombre d'essais: {n_trials}\")\n",
    "print(f\"   Nombre de neurones: {n_neurons}\")\n",
    "print(f\"   Nombre de classes: {n_classes}\")\n",
    "print(f\"\\n   Distribution des classes:\")\n",
    "for i in range(n_classes):\n",
    "    count = np.sum(y_multi == i)\n",
    "    print(f\"      Direction {i}: {count} √©chantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division en ensembles d'entra√Ænement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les donn√©es\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "print(f\"üìä Division:\")\n",
    "print(f\"   Entra√Ænement: {X_train_multi.shape[0]} √©chantillons\")\n",
    "print(f\"   Test: {X_test_multi.shape[0]} √©chantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement du mod√®le multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner un mod√®le de r√©gression logistique multi-classe\n",
    "model_multi = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)\n",
    "model_multi.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_train_pred_multi = model_multi.predict(X_train_multi)\n",
    "y_test_pred_multi = model_multi.predict(X_test_multi)\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy_multi = accuracy_score(y_train_multi, y_train_pred_multi)\n",
    "test_accuracy_multi = accuracy_score(y_test_multi, y_test_pred_multi)\n",
    "\n",
    "print(f\"üìà Performance:\")\n",
    "print(f\"   Accuracy (entra√Ænement): {train_accuracy_multi:.4f} ({train_accuracy_multi*100:.2f}%)\")\n",
    "print(f\"   Accuracy (test): {test_accuracy_multi:.4f} ({test_accuracy_multi*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion pour multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm_multi = confusion_matrix(y_test_multi, y_test_pred_multi)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "direction_labels = ['Droite', 'Gauche', 'Haut', 'Bas']\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=direction_labels,\n",
    "            yticklabels=direction_labels,\n",
    "            cbar_kws={'label': 'Nombre'})\n",
    "plt.xlabel('Pr√©diction', fontsize=12)\n",
    "plt.ylabel('Vraie direction', fontsize=12)\n",
    "plt.title('Matrice de confusion - Classification de directions', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation:\")\n",
    "print(\"   ‚Ä¢ La diagonale montre les pr√©dictions correctes\")\n",
    "print(\"   ‚Ä¢ Les √©l√©ments hors-diagonale sont les erreurs\")\n",
    "print(\"   ‚Ä¢ On peut identifier quelles directions sont confondues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapport de classification d√©taill√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Rapport de classification d√©taill√©:\\n\")\n",
    "print(classification_report(y_test_multi, y_test_pred_multi,\n",
    "                          target_names=direction_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des poids : Quels neurones sont importants ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer les poids\n",
    "weights_multi = model_multi.coef_  # Shape: (n_classes, n_features)\n",
    "\n",
    "# Visualisation des poids pour chaque classe\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    axes[i].bar(range(n_neurons), weights_multi[i], alpha=0.7)\n",
    "    axes[i].set_xlabel('Neurone', fontsize=11)\n",
    "    axes[i].set_ylabel('Poids', fontsize=11)\n",
    "    axes[i].set_title(f'{direction_labels[i]}', fontsize=12)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "    axes[i].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation des poids:\")\n",
    "print(\"   ‚Ä¢ Poids positifs: augmentation de l'activit√© favorise cette direction\")\n",
    "print(\"   ‚Ä¢ Poids n√©gatifs: augmentation de l'activit√© d√©favorise cette direction\")\n",
    "print(\"   ‚Ä¢ Poids proches de 0: neurone peu informatif pour cette direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification des neurones les plus informatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer l'importance globale de chaque neurone\n",
    "# (moyenne de la valeur absolue des poids √† travers toutes les classes)\n",
    "neuron_importance = np.mean(np.abs(weights_multi), axis=0)\n",
    "sorted_neurons = np.argsort(neuron_importance)[::-1]\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(n_neurons), neuron_importance[sorted_neurons], alpha=0.7)\n",
    "plt.yticks(range(n_neurons), [f'Neurone {i+1}' for i in sorted_neurons])\n",
    "plt.xlabel('Importance moyenne (|poids|)', fontsize=12)\n",
    "plt.title('Importance des neurones pour la classification', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüß† Top 5 neurones les plus informatifs:\")\n",
    "for rank, idx in enumerate(sorted_neurons[:5]):\n",
    "    print(f\"   {rank+1}. Neurone {idx+1}: importance = {neuron_importance[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercices pratiques\n",
    "\n",
    "### √Ä vous de jouer !\n",
    "\n",
    "**Exercice 1 :** Modifier le seuil de d√©cision\n",
    "- Au lieu d'utiliser 0.5 comme seuil, essayez 0.3 ou 0.7\n",
    "- Comment cela affecte-t-il l'accuracy et la matrice de confusion ?\n",
    "\n",
    "**Exercice 2 :** Augmenter le nombre de directions\n",
    "- Modifier n_classes √† 8 (8 directions)\n",
    "- R√©entra√Æner le mod√®le\n",
    "- Comment la performance change-t-elle ?\n",
    "\n",
    "**Exercice 3 :** Feature importance\n",
    "- Identifier les 5 neurones les plus importants\n",
    "- Entra√Æner un nouveau mod√®le avec seulement ces neurones\n",
    "- Comparer la performance\n",
    "\n",
    "**Exercice 4 :** Donn√©es non lin√©airement s√©parables\n",
    "- Cr√©er un dataset circulaire (make_circles de sklearn)\n",
    "- Essayer la r√©gression logistique lin√©aire\n",
    "- Observer les limites du mod√®le lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons :\n",
    "\n",
    "‚úÖ Compris la fonction sigmo√Øde et son r√¥le dans la classification\n",
    "\n",
    "‚úÖ Impl√©ment√© la r√©gression logistique from scratch\n",
    "\n",
    "‚úÖ Visualis√© les fronti√®res de d√©cision\n",
    "\n",
    "‚úÖ Utilis√© scikit-learn pour une impl√©mentation robuste\n",
    "\n",
    "‚úÖ √âtendu √† la classification multi-classe\n",
    "\n",
    "‚úÖ Appliqu√© √† un probl√®me de neurosciences\n",
    "\n",
    "‚úÖ Appris √† interpr√©ter les matrices de confusion\n",
    "\n",
    "‚úÖ Analys√© l'importance des features (neurones)\n",
    "\n",
    "### Points cl√©s √† retenir:\n",
    "\n",
    "1. **La r√©gression logistique** utilise la sigmo√Øde pour transformer les sorties en probabilit√©s\n",
    "2. **Binary cross-entropy** est la fonction de co√ªt appropri√©e pour la classification\n",
    "3. **La fronti√®re de d√©cision** est une ligne (ou hyperplan) qui s√©pare les classes\n",
    "4. **Matrice de confusion** permet de voir les types d'erreurs du mod√®le\n",
    "5. **Multi-classe** utilise softmax et categorical cross-entropy\n",
    "6. **Les poids** indiquent l'importance des features pour chaque classe\n",
    "\n",
    "### Prochaine √©tape:\n",
    "Passez au notebook **4.3_exercices.ipynb** pour plus de pratique!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
