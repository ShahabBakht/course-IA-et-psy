{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 R√©gression Lin√©aire en Pratique\n",
    "\n",
    "## PSY3913/6913 - IA, Psychologie et Neuroscience Cognitive\n",
    "\n",
    "Dans ce notebook, nous allons explorer la r√©gression lin√©aire de mani√®re pratique :\n",
    "1. Cr√©er des donn√©es synth√©tiques\n",
    "2. Impl√©menter la r√©gression lin√©aire from scratch\n",
    "3. Visualiser les r√©sultats\n",
    "4. Utiliser scikit-learn\n",
    "5. Appliquer √† un exemple de neurosciences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pour des graphiques plus jolis\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Pour la reproductibilit√©\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cr√©ation de donn√©es synth√©tiques simples\n",
    "\n",
    "Commen√ßons avec un exemple simple en 1D : pr√©dire y √† partir de x.\n",
    "\n",
    "Nous allons g√©n√©rer des donn√©es suivant la relation : **y = 2x + 1 + bruit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es synth√©tiques\n",
    "n_samples = 100\n",
    "X = np.random.rand(n_samples, 1) * 10  # Valeurs entre 0 et 10\n",
    "y_true = 2 * X + 1  # Vraie relation lin√©aire\n",
    "noise = np.random.randn(n_samples, 1) * 2  # Bruit gaussien\n",
    "y = y_true + noise  # Donn√©es observ√©es\n",
    "\n",
    "print(f\"Forme de X: {X.shape}\")\n",
    "print(f\"Forme de y: {y.shape}\")\n",
    "print(f\"\\nPremiers √©chantillons:\")\n",
    "print(f\"X[:5] = {X[:5].flatten()}\")\n",
    "print(f\"y[:5] = {y[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Donn√©es observ√©es')\n",
    "plt.plot(X, y_true, 'r-', linewidth=2, label='Vraie relation (sans bruit)')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Donn√©es synth√©tiques pour la r√©gression lin√©aire', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: Les points sont dispers√©s autour de la ligne rouge (vraie relation)\")\n",
    "print(\"   Le bruit rend les donn√©es plus r√©alistes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impl√©mentation from scratch : Descente de gradient\n",
    "\n",
    "Nous allons impl√©menter la r√©gression lin√©aire en utilisant la descente de gradient.\n",
    "\n",
    "**Rappel :**\n",
    "- Mod√®le : $\\hat{y} = wx + b$\n",
    "- Loss (MSE) : $L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "- Gradients : \n",
    "  - $\\frac{\\partial L}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) x_i$\n",
    "  - $\\frac{\\partial L}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    R√©gression lin√©aire avec descente de gradient.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Entra√Æner le mod√®le.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialisation al√©atoire des param√®tres\n",
    "        self.w = np.random.randn(n_features, 1) * 0.01\n",
    "        self.b = 0\n",
    "        \n",
    "        # Descente de gradient\n",
    "        for i in range(self.n_iterations):\n",
    "            # Pr√©diction\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Calcul de la loss\n",
    "            loss = np.mean((y - y_pred) ** 2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Calcul des gradients\n",
    "            dw = -(2 / n_samples) * X.T.dot(y - y_pred)\n",
    "            db = -(2 / n_samples) * np.sum(y - y_pred)\n",
    "            \n",
    "            # Mise √† jour des param√®tres\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Affichage occasionnel\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"It√©ration {i+1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Faire des pr√©dictions.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w) + self.b\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Retourner les param√®tres appris.\n",
    "        \"\"\"\n",
    "        return {'w': self.w, 'b': self.b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et entra√Æner le mod√®le\n",
    "model_scratch = LinearRegressionGD(learning_rate=0.01, n_iterations=1000)\n",
    "print(\"Entra√Ænement du mod√®le...\\n\")\n",
    "model_scratch.fit(X, y)\n",
    "\n",
    "# R√©cup√©rer les param√®tres appris\n",
    "params = model_scratch.get_params()\n",
    "print(f\"\\nüìä Param√®tres appris:\")\n",
    "print(f\"   w (poids) = {params['w'][0, 0]:.4f}\")\n",
    "print(f\"   b (biais) = {params['b']:.4f}\")\n",
    "print(f\"\\nüéØ Param√®tres vrais:\")\n",
    "print(f\"   w = 2.0000\")\n",
    "print(f\"   b = 1.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de la courbe d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model_scratch.loss_history, linewidth=2)\n",
    "plt.xlabel('It√©ration', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Courbe d\\'apprentissage - √âvolution de la loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: La loss diminue progressivement et converge.\")\n",
    "print(\"   C'est un signe que l'apprentissage fonctionne bien!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "y_pred_scratch = model_scratch.predict(X)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Donn√©es observ√©es')\n",
    "plt.plot(X, y_true, 'r-', linewidth=2, label='Vraie relation', alpha=0.5)\n",
    "plt.plot(X, y_pred_scratch, 'g-', linewidth=2, label='Pr√©dictions (from scratch)')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('R√©gression lin√©aire - Comparaison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calcul des m√©triques\n",
    "mse_scratch = np.mean((y - y_pred_scratch) ** 2)\n",
    "r2_scratch = 1 - (np.sum((y - y_pred_scratch) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
    "\n",
    "print(f\"\\nüìà Performance du mod√®le (from scratch):\")\n",
    "print(f\"   MSE = {mse_scratch:.4f}\")\n",
    "print(f\"   R¬≤ = {r2_scratch:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utilisation de scikit-learn\n",
    "\n",
    "Maintenant, comparons avec l'impl√©mentation de scikit-learn qui utilise les √©quations normales (solution analytique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et entra√Æner le mod√®le sklearn\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X, y)\n",
    "\n",
    "# R√©cup√©rer les param√®tres\n",
    "w_sklearn = model_sklearn.coef_[0, 0]\n",
    "b_sklearn = model_sklearn.intercept_[0]\n",
    "\n",
    "print(f\"üìä Param√®tres scikit-learn:\")\n",
    "print(f\"   w (poids) = {w_sklearn:.4f}\")\n",
    "print(f\"   b (biais) = {b_sklearn:.4f}\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_sklearn = model_sklearn.predict(X)\n",
    "\n",
    "# M√©triques\n",
    "mse_sklearn = mean_squared_error(y, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y, y_pred_sklearn)\n",
    "\n",
    "print(f\"\\nüìà Performance du mod√®le (scikit-learn):\")\n",
    "print(f\"   MSE = {mse_sklearn:.4f}\")\n",
    "print(f\"   R¬≤ = {r2_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des deux approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, alpha=0.5, label='Donn√©es observ√©es')\n",
    "plt.plot(X, y_pred_scratch, 'g-', linewidth=2, label='From scratch (descente de gradient)', alpha=0.7)\n",
    "plt.plot(X, y_pred_sklearn, 'b--', linewidth=2, label='Scikit-learn (√©quations normales)', alpha=0.7)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Comparaison : From scratch vs Scikit-learn', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: Les deux approches donnent des r√©sultats tr√®s similaires!\")\n",
    "print(\"   Les petites diff√©rences sont dues √† l'approximation de la descente de gradient.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exemple appliqu√© : Neurosciences\n",
    "\n",
    "### Probl√®me : Pr√©dire la vitesse de mouvement √† partir de l'activit√© neuronale\n",
    "\n",
    "Simulons des donn√©es r√©alistes o√π :\n",
    "- **Entr√©es (X)** : Activit√© de 10 neurones (firing rates en Hz)\n",
    "- **Sortie (y)** : Vitesse de mouvement (cm/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler des donn√©es de neurosciences\n",
    "np.random.seed(123)\n",
    "\n",
    "n_trials = 200  # Nombre d'essais\n",
    "n_neurons = 10  # Nombre de neurones enregistr√©s\n",
    "\n",
    "# G√©n√©rer l'activit√© neuronale (firing rates entre 0 et 50 Hz)\n",
    "X_neuro = np.random.rand(n_trials, n_neurons) * 50\n",
    "\n",
    "# Simuler la relation : certains neurones contribuent plus que d'autres\n",
    "# Poids vrais : certains neurones sont plus informatifs\n",
    "true_weights = np.array([0.5, 0.8, 0.2, 0.1, 0.6, 0.3, 0.7, 0.4, 0.1, 0.5])\n",
    "true_bias = 10.0\n",
    "\n",
    "# Calculer la vitesse avec du bruit\n",
    "y_neuro_true = X_neuro.dot(true_weights.reshape(-1, 1)) + true_bias\n",
    "noise_neuro = np.random.randn(n_trials, 1) * 5\n",
    "y_neuro = y_neuro_true + noise_neuro\n",
    "\n",
    "print(f\"üìä Dataset de neurosciences:\")\n",
    "print(f\"   Nombre d'essais: {n_trials}\")\n",
    "print(f\"   Nombre de neurones: {n_neurons}\")\n",
    "print(f\"   Forme de X: {X_neuro.shape}\")\n",
    "print(f\"   Forme de y: {y_neuro.shape}\")\n",
    "print(f\"\\n   Vitesse moyenne: {y_neuro.mean():.2f} cm/s\")\n",
    "print(f\"   Vitesse std: {y_neuro.std():.2f} cm/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division en ensembles d'entra√Ænement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les donn√©es : 80% entra√Ænement, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_neuro, y_neuro, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Division des donn√©es:\")\n",
    "print(f\"   Entra√Ænement: {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"   Test: {X_test.shape[0]} √©chantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le\n",
    "model_neuro = LinearRegression()\n",
    "model_neuro.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_train_pred = model_neuro.predict(X_train)\n",
    "y_test_pred = model_neuro.predict(X_test)\n",
    "\n",
    "# M√©triques\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"üìà Performance sur l'ensemble d'entra√Ænement:\")\n",
    "print(f\"   MSE = {train_mse:.4f}\")\n",
    "print(f\"   R¬≤ = {train_r2:.4f}\")\n",
    "print(f\"\\nüìà Performance sur l'ensemble de test:\")\n",
    "print(f\"   MSE = {test_mse:.4f}\")\n",
    "print(f\"   R¬≤ = {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation : Pr√©dictions vs Vraies valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Ensemble d'entra√Ænement\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.6)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "             'r--', linewidth=2, label='Ligne parfaite (y=≈∑)')\n",
    "axes[0].set_xlabel('Vitesse r√©elle (cm/s)', fontsize=12)\n",
    "axes[0].set_ylabel('Vitesse pr√©dite (cm/s)', fontsize=12)\n",
    "axes[0].set_title(f'Ensemble d\\'entra√Ænement\\nR¬≤ = {train_r2:.3f}', fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble de test\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, color='green')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Ligne parfaite (y=≈∑)')\n",
    "axes[1].set_xlabel('Vitesse r√©elle (cm/s)', fontsize=12)\n",
    "axes[1].set_ylabel('Vitesse pr√©dite (cm/s)', fontsize=12)\n",
    "axes[1].set_title(f'Ensemble de test\\nR¬≤ = {test_r2:.3f}', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation: Les points sont bien align√©s sur la ligne y=≈∑,\")\n",
    "print(\"   ce qui indique que le mod√®le fait de bonnes pr√©dictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des poids : Quels neurones sont importants ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer les poids appris\n",
    "learned_weights = model_neuro.coef_[0]\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Comparaison poids vrais vs appris\n",
    "x_pos = np.arange(n_neurons)\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, true_weights, width, label='Poids vrais', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, learned_weights, width, label='Poids appris', alpha=0.8)\n",
    "axes[0].set_xlabel('Neurone', fontsize=12)\n",
    "axes[0].set_ylabel('Poids', fontsize=12)\n",
    "axes[0].set_title('Comparaison des poids', fontsize=13)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([f'N{i+1}' for i in range(n_neurons)])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Importance relative (valeur absolue)\n",
    "importance = np.abs(learned_weights)\n",
    "sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "axes[1].barh(range(n_neurons), importance[sorted_idx], alpha=0.8)\n",
    "axes[1].set_yticks(range(n_neurons))\n",
    "axes[1].set_yticklabels([f'Neurone {i+1}' for i in sorted_idx])\n",
    "axes[1].set_xlabel('Importance (|poids|)', fontsize=12)\n",
    "axes[1].set_title('Importance relative des neurones', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüß† Analyse des neurones:\")\n",
    "print(f\"\\n   Top 3 neurones les plus informatifs:\")\n",
    "for i, idx in enumerate(sorted_idx[:3]):\n",
    "    print(f\"   {i+1}. Neurone {idx+1}: poids = {learned_weights[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des r√©sidus (erreurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les r√©sidus\n",
    "residuals_train = y_train - y_train_pred\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# R√©sidus vs Pr√©dictions (entra√Ænement)\n",
    "axes[0, 0].scatter(y_train_pred, residuals_train, alpha=0.6)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Pr√©dictions', fontsize=11)\n",
    "axes[0, 0].set_ylabel('R√©sidus', fontsize=11)\n",
    "axes[0, 0].set_title('R√©sidus vs Pr√©dictions (entra√Ænement)', fontsize=12)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# R√©sidus vs Pr√©dictions (test)\n",
    "axes[0, 1].scatter(y_test_pred, residuals_test, alpha=0.6, color='green')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Pr√©dictions', fontsize=11)\n",
    "axes[0, 1].set_ylabel('R√©sidus', fontsize=11)\n",
    "axes[0, 1].set_title('R√©sidus vs Pr√©dictions (test)', fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des r√©sidus (entra√Ænement)\n",
    "axes[1, 0].hist(residuals_train, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('R√©sidus', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Fr√©quence', fontsize=11)\n",
    "axes[1, 0].set_title('Distribution des r√©sidus (entra√Ænement)', fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Distribution des r√©sidus (test)\n",
    "axes[1, 1].hist(residuals_test, bins=15, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('R√©sidus', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Fr√©quence', fontsize=11)\n",
    "axes[1, 1].set_title('Distribution des r√©sidus (test)', fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   1. Les r√©sidus sont distribu√©s de mani√®re al√©atoire autour de 0\")\n",
    "print(\"   2. Pas de pattern visible ‚Üí le mod√®le lin√©aire est appropri√©\")\n",
    "print(\"   3. Distribution approximativement gaussienne ‚Üí hypoth√®ses respect√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercice pratique\n",
    "\n",
    "### √Ä vous de jouer ! \n",
    "\n",
    "**Exercice :** Modifiez le code ci-dessus pour :\n",
    "1. Changer le nombre de neurones (essayez avec 5 ou 20 neurones)\n",
    "2. Augmenter le bruit dans les donn√©es\n",
    "3. Observer comment la performance (R¬≤) change\n",
    "\n",
    "**Questions de r√©flexion :**\n",
    "- Que se passe-t-il si vous avez tr√®s peu de donn√©es d'entra√Ænement ?\n",
    "- Comment la performance change-t-elle avec plus de neurones ?\n",
    "- Que se passe-t-il si certains neurones ne sont pas informatifs (poids = 0) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons :\n",
    "\n",
    "‚úÖ Impl√©ment√© la r√©gression lin√©aire from scratch avec la descente de gradient\n",
    "\n",
    "‚úÖ Utilis√© scikit-learn pour une impl√©mentation robuste\n",
    "\n",
    "‚úÖ Appliqu√© la r√©gression √† un probl√®me de neurosciences\n",
    "\n",
    "‚úÖ Appris √† √©valuer et visualiser les performances\n",
    "\n",
    "‚úÖ Interpr√©t√© les poids pour comprendre l'importance des features\n",
    "\n",
    "### Points cl√©s √† retenir:\n",
    "\n",
    "1. **La r√©gression lin√©aire** est un mod√®le simple mais puissant pour pr√©dire des valeurs continues\n",
    "2. **L'√©valuation** doit se faire sur un ensemble de test s√©par√©\n",
    "3. **R¬≤** mesure la proportion de variance expliqu√©e (0 √† 1, plus c'est √©lev√©, mieux c'est)\n",
    "4. **Les r√©sidus** doivent √™tre distribu√©s al√©atoirement autour de 0\n",
    "5. **Les poids** indiquent l'importance relative des features\n",
    "\n",
    "### Prochaine √©tape:\n",
    "Passez au notebook **4.2_classification_lineaire.ipynb** pour explorer la classification lin√©aire!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
